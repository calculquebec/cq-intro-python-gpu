{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sapphire-virgin",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Solutions to notebook 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644c215-cf8a-41b4-a08b-d1fdd037d8d6",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Lets do the following exercise where each element of an array is incremented : array[i] = array[i] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def kernel1(array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos<array.size:\n",
    "        array[pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=numpy.ones(12800,dtype=np.int32)\n",
    "threads=32\n",
    "blocks = (data.size + (threads - 1)) // threads\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kernel and measure execution time:\n",
    "%timeit kernel1[blocks,threads](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take advatage of excplicit data management and copy an array to GPU before kernel execution. \n",
    "# Then measure the execution time again\n",
    "d_data = cuda.to_device(data)\n",
    "%timeit kernel1[blocks,threads](d_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a45c30-e253-4788-88e9-b12c49f98629",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Here an integer array is sent to GPU where its indices are reversed, i.e. array[0]=array[N-1], array[1]=array[N-2], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import numpy as np\n",
    "from numba import cuda, float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b968561-d26a-4592-81b0-4c7c815aeb61",
   "metadata": {},
   "source": [
    "Here we re-use the code from Ex.2 and add shared memory into play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import numpy as np\n",
    "from numba import cuda, float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91c746-ae84-41ea-83ad-f06dbd6545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take this code and re-write it in the next cell by using a shared memory \n",
    "@cuda.jit\n",
    "def reverseArrayBlock(d_out,d_in):\n",
    "    ind_in = cuda.blockDim.x*cuda.blockIdx.x + cuda.threadIdx.x; ## Index of the current thread\n",
    "    ind_out = cuda.gridsize(1)-ind_in-1 ## Total number of threads - in -1\n",
    "    if ind_in<d_in.size:\n",
    "        d_out[ind_out] = d_in[ind_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84db54f-76c5-482a-8c9d-53fbb07cf2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the code with shared memory\n",
    "@cuda.jit\n",
    "def reverseArrayBlock_shared(d_out,d_in):\n",
    "# Static shared memory declaration\n",
    "#    s = cuda.shared.array(2000, dtype=int32) \n",
    "    # Below is dynamic shared memory declaration\n",
    "    s = cuda.shared.array(0, dtype=int32) \n",
    "    ind_in = cuda.blockDim.x*cuda.blockIdx.x + cuda.threadIdx.x; ## Index of the current thread\n",
    "    ind_out = cuda.gridsize(1)-ind_in-1 ## Total number of threads - in -1\n",
    "    s[cuda.blockDim.x - cuda.threadIdx.x - 1] = d_in[ind_in]\n",
    "    cuda.syncthreads()\n",
    "    ind_out = cuda.blockDim.x*(cuda.gridDim.x - 1 - cuda.blockIdx.x) + cuda.threadIdx.x\n",
    "    if ind_in<d_in.size:\n",
    "        d_out[ind_out] = s[cuda.threadIdx.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afbc0b-2678-430d-a489-f1d259157726",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=256*1000\n",
    "NumThreadsPerBlock=128\n",
    "NumBlocks = (dim + (NumThreadsPerBlock - 1)) // NumThreadsPerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba551a3-eb54-48ed-af5c-0e96667f071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0,dim,dtype=np.int32)\n",
    "b = np.zeros(dim,dtype=np.int32)\n",
    "memSize = NumThreadsPerBlock * a.dtype.itemsize\n",
    "print(memSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a2fc1-e0b9-4bc1-8d5d-02512be342ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static shared memory call\n",
    "# reverseArrayBlock_shared[NumBlocks,NumThreadsPerBlock](b,a)\n",
    "#Dynamic shared memory call \n",
    "reverseArrayBlock_shared[NumBlocks,NumThreadsPerBlock,0,memSize](b,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff1b2c-6429-4650-9d72-62b55183bc06",
   "metadata": {},
   "source": [
    "### Hands-on: Matrix multiplication with shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da6fae-c2c2-4bab-90d0-8f5ae6dad59f",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](../images/05-matmulshared.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fddc5-16c4-49bf-8efc-ace8a1bbdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b2701-db0b-4510-83cd-c2024d963976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "def fast_matmul(A, B, C):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    \n",
    "    # Define global and thread indices\n",
    "    \n",
    "    # Define number of blocks per grid\n",
    "    \n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        #####\n",
    "        \n",
    "        # Wait until all threads finish preloading\n",
    "        \n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            #####\n",
    "            \n",
    "        # Wait until all threads finish computing\n",
    "        \n",
    "    # Put tmp into C matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2279b5-9394-4652-9425-d6e5e262aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create matrices A,B,C as numpy arrays (size 128x128,float32). Fill A and B with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51877ee2-1e83-48c0-8a22-4f8ad9c9c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Calculate number of blocks and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa23303-e3bd-4e56-8788-01e89fb80ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function and time it to get the execution time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
