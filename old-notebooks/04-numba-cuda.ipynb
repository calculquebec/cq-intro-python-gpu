{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sapphire-virgin",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Numba + CUDA\n",
    "\n",
    "Universal functions are great for element wise operations. \n",
    "However, not all operations are element wise. To compile a function on the GPU that is not element wise, we must use `numba.cuda.jit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939b280-8a64-480a-8af4-6885105255a2",
   "metadata": {},
   "source": [
    "#### CUDA terminology\n",
    "Before we jump into CUDA with Python lets talk about CUDA terminology and main execution concept:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c1623-25a5-49c7-9f3c-74b8b23635c9",
   "metadata": {},
   "source": [
    "![](images/host_device.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6096fb8e-8091-4fc2-9ae3-8f469d1318fc",
   "metadata": {},
   "source": [
    "#### CUDA kernel\n",
    "We have been talking about CUDA kernels, but what is CUDA kernel ? \n",
    "![](images/cuda_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa71fbd-d324-4276-acec-51cf0c773bab",
   "metadata": {},
   "source": [
    "In CUDA we divide a program into a grid of threads, and a kernel is a program executed on each of those threads independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b19f9-deb8-482a-b82c-e8068e8433b0",
   "metadata": {},
   "source": [
    "It's different from how we create a CPU program as there we have to explicitate every operation, every loop, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72760b-2975-41ee-a4f0-d5672d4a642a",
   "metadata": {},
   "source": [
    "Lets look at the matrix addition example.\n",
    "In the CPU implementation we would loop over all the elements of matrix A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc89712-cad3-4626-a10d-15979b56a455",
   "metadata": {},
   "source": [
    "![](images/matrix_cpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c238f-9e74-4b83-9ffe-5c376a9dbf6d",
   "metadata": {},
   "source": [
    "![](images/matrix_gpu2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227dd4e-d364-4388-9aae-222cc0dace03",
   "metadata": {},
   "source": [
    "Unfortunately there is another layer of complexity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0f69a-bad7-48b1-b1b6-c14d65d2a533",
   "metadata": {},
   "source": [
    "![](images/cuda_block_grid2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fa32b-66b6-45d9-b4b6-2cc966d50361",
   "metadata": {},
   "source": [
    "### CUDA kernel declaration\n",
    "Once again, here is how to declare a kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \"\"\"\n",
    "    Code for kernel.\n",
    "    \"\"\"\n",
    "    # code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data array \n",
    "data=numpy.ones(12800)\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# Now finally start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aeeb05",
   "metadata": {},
   "source": [
    "### Choosing the block size\n",
    "\n",
    "* On the software side, the block size determines how many threads share a given area of shared memory.\n",
    "* On the hardware side, the block size must be large enough for full occupation of execution units; \n",
    "The block size you choose depends on:\n",
    "* The size of the data array\n",
    "* The size of the shared mempory per block (e.g. 64KB)\n",
    "* The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)\n",
    "* The maximum number of threads per multiprocessor (MP) (e.g. 2048)\n",
    "* The maximum number of blocks per MP (e.g. 32)\n",
    "* The number of threads that can be executed concurrently (a “warp” i.e. 32)\n",
    "\n",
    "Rules of thumb for threads per block:\n",
    "\n",
    "    Should be a round multiple of the warp size (32)\n",
    "    A good place to start is 128-512 but benchmarking is required to determine the optimal value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-quarter",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Lets do the following exercise where each element of an array is incremented : array[i] = array[i] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libs\n",
    "import ...\n",
    "from numba import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a GPU code (Kernel)\n",
    "def kernel1(array):\n",
    "    #define thread index i here ...\n",
    "    if i<array.size:\n",
    "        array[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid: provide with number of blocks and threads per block\n",
    "data=numpy.ones(12800)\n",
    "threads=32\n",
    "blocks ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kernel and measure execution time:\n",
    "kernel1[blocks,threads](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take advatage of excplicit data management and copy an array to GPU before kernel execution. \n",
    "# Then measure the execution time again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7bb8e-7ad0-4871-a7a1-e51ffb2a8d49",
   "metadata": {},
   "source": [
    "### Explicit data management\n",
    "\n",
    "Numba has been automatically transferring the NumPy arrays to the device when you invoke the kernel. However, it can only do so conservatively by always transferring the device memory back to the host when a kernel finishes. To avoid the unnecessary transfer for read-only arrays, it is possible to manually control the transfer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701de911-6b8f-4bb3-9e87-b43b43bc72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.device_array( shape ) #Allocates an empty device ndarray. Similar to numpy.empty()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045d469-fe8b-4118-9023-73139e23c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.to_device( array ) #Copy data from CPU array to GPU array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594a7ea-071e-439c-9199-021ae3711108",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = device_array.copy_to_host() #Copy data back to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175833c-77e6-48fe-bf4f-120674507ee4",
   "metadata": {},
   "source": [
    "Now go back to exercise 1 and modify the code by using the expicit data management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-lighter",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Here an integer array is sent to GPU where its indices are reversed, i.e. array[0]=array[N-1], array[1]=array[N-2], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator\n",
    "# Kernel: reverse the array content using appropriate indices. \n",
    "# To do so you may need input and output indices. Implement kernel with possibility of multiple thread blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid\n",
    "dim=256*1000\n",
    "NumThreadsPerBlock=\n",
    "NumBlocks = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create arrays on CPU and GPU (if you want to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Initialize host array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Verify the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06a577",
   "metadata": {},
   "source": [
    "### Hands-on: Matrix multiplication on GPU (with global memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b242200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(A,B,C):\n",
    "    # iterating by row of A\n",
    "    for i in range(len(A)):\n",
    "  \n",
    "        # iterating by coloum by B \n",
    "        for j in range(len(B[0])):\n",
    "  \n",
    "            # iterating by rows of B\n",
    "            for k in range(len(B)):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f7372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Part 1: Create matrices A,B,C as numpy arrays. Fill A and B with random numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da28fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Calculate number of blocks and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d64235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function and time it to get the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Create A,B,C manually on the GPU and copy data to the GPU arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Call the kernel function and time it to get the execution time. Compare the execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2c420",
   "metadata": {},
   "source": [
    "### Shared memory\n",
    "A limited amount of shared memory can be allocated on the device to speed up access to data. That memory is shared amongst all threads in a given block. It's so much faster than the regular device memory. It also allows threads to cooperate on a given solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " numba.cuda.shared.array(shape, type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07371739",
   "metadata": {},
   "source": [
    "This function is called on the device, i.e. from the kernel or device function. A common pattern is to have each thread populate one element in the shared array, then wait for all threads to finish using syncthtreads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50deab",
   "metadata": {},
   "outputs": [],
   "source": [
    " numba.cuda.syncthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1bd67-8421-4925-bb63-d73f7deb30fa",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b968561-d26a-4592-81b0-4c7c815aeb61",
   "metadata": {},
   "source": [
    "Here we re-use the code from Ex.2 and add shared memory into play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91c746-ae84-41ea-83ad-f06dbd6545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take this code and re-write it in the next cell by using a shared memory \n",
    "@cuda.jit\n",
    "def reverseArrayBlock(d_out,d_in):\n",
    "    ind_in = cuda.blockDim.x*cuda.blockIdx.x + cuda.threadIdx.x; ## Index of the current thread\n",
    "    ind_out = cuda.gridsize(1)-ind_in-1 ## Total number of threads - in -1\n",
    "    if ind_in<d_in.size:\n",
    "        d_out[ind_out] = d_in[ind_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84db54f-76c5-482a-8c9d-53fbb07cf2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Here is the code with shared memory\n",
    "@cuda.jit\n",
    "def reverseArrayBlock_shared(d_out,d_in):\n",
    "    # Declare/allocate array s in shared memory\n",
    "    ....\n",
    "    # Create input index\n",
    "    ....\n",
    "    # Populate array s from arrat d_in\n",
    "    ....\n",
    "    # Synchronize threads in each block\n",
    "    ....\n",
    "    # Create output index\n",
    "    ....\n",
    "    if ind_in<d_in.size:\n",
    "        # Populate output array d_out from shared array s\n",
    "        ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afbc0b-2678-430d-a489-f1d259157726",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=256*1000\n",
    "NumThreadsPerBlock=128\n",
    "NumBlocks = (dim + (NumThreadsPerBlock - 1)) // NumThreadsPerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba551a3-eb54-48ed-af5c-0e96667f071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create arrays on CPU and GPU (if you want to)\n",
    "a = np.arange(0,dim,dtype=np.int32)\n",
    "b = np.zeros(dim,dtype=np.int32)\n",
    "print(memSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a2fc1-e0b9-4bc1-8d5d-02512be342ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Call the kernel\n",
    "reverseArrayBlock_shared[NumBlocks,NumThreadsPerBlock,0,memSize](b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dffca0-ec57-4a26-af78-cb356d86fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Modify the kernel as well as the call from the host by changing static shared memory declaration to dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff1b2c-6429-4650-9d72-62b55183bc06",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hands-on: Matrix multiplication with shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da6fae-c2c2-4bab-90d0-8f5ae6dad59f",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](images/05-matmulshared.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fddc5-16c4-49bf-8efc-ace8a1bbdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b2701-db0b-4510-83cd-c2024d963976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "def fast_matmul(A, B, C):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    \n",
    "    # Define global and thread indices\n",
    "    \n",
    "    # Define number of blocks per grid\n",
    "    \n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        #####\n",
    "        \n",
    "        # Wait until all threads finish preloading\n",
    "        \n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            #####\n",
    "            \n",
    "        # Wait until all threads finish computing\n",
    "        \n",
    "    # Put tmp into C matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2279b5-9394-4652-9425-d6e5e262aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create matrices A,B,C as numpy arrays (size 128x128). Fill A and B with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51877ee2-1e83-48c0-8a22-4f8ad9c9c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Calculate number of blocks and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa23303-e3bd-4e56-8788-01e89fb80ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function and time it to get the execution time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
