{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a724b2-cd1f-41d7-b086-d9e221d869bd",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Alternatives to Numba: CuPy and Pycuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25c3a6-b64c-4dad-baf4-4daa53765556",
   "metadata": {},
   "source": [
    "### CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48318f3d-6306-43b7-9f87-5d5efa25d349",
   "metadata": {},
   "source": [
    "CuPy is a GPU array backend that implements a subset of NumPy interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750c3fc-3082-49ab-9646-e77b2d33fa14",
   "metadata": {},
   "source": [
    "First load NumPu and CuPy modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349126f-c54a-4230-9383-d865e0f838b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install cupy --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006c980-04df-4e30-8881-868ddb5fd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f9252-bb0a-4471-8211-44a10ea720a5",
   "metadata": {},
   "source": [
    "Now we create matrices A,B,C in CPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df0cee-a8b4-4c91-aac5-7b1c0572402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.rand(512,512).astype(np.float32)\n",
    "B=np.random.rand(512,512).astype(np.float32)\n",
    "C=np.zeros(shape=(512,512)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e695c4e-86b5-4e01-8b65-84d904d9462e",
   "metadata": {},
   "source": [
    "Then we copy data to GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18edad9-8800-41bf-8744-8e7e7bdc796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_A=cp.asarray(A)\n",
    "d_B=cp.asarray(B)\n",
    "d_C=cp.asarray(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880de4a-23c4-4416-bbbc-6e605e3b6649",
   "metadata": {},
   "source": [
    "Then we use Numpy built-in matrix multiplication function and run on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4a7f4-d704-4e69-9a50-ff8a8ca4f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit C = np.matmul(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed09b9-1caf-4eae-b0cf-fee6f96be44d",
   "metadata": {},
   "source": [
    "Then we use CuPy built-in matrix multiplication function and run on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ee75c-78dc-4e23-9a67-b70977c9358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit d_C = cp.matmul(d_A,d_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcf780-8dba-4111-88ee-6158261609db",
   "metadata": {},
   "source": [
    "### PyCuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcee9d-bb51-4f5f-9066-2ee0dffac684",
   "metadata": {},
   "source": [
    "PyCuda gives you easy, Pythonic access to Nvidia's CUDA parallel computation API.\n",
    "The idea is that you write CUDA kernels in C/C++, use a wrapper to make it Python object, but variables and execution are managed by Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f267aa1-8fa3-4803-9d92-1921a22843e8",
   "metadata": {},
   "source": [
    "First load all pycuda modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60645ee6-82a9-4e15-b642-70210855b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycuda import compiler\n",
    "from pycuda import driver as cuda\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1573752-183f-4026-af0c-2ed38108f336",
   "metadata": {},
   "source": [
    "Write CUDA C code and feed it into the constructor of a pycuda.compiler.SourceModule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2d8c8-e2d1-4669-bcb9-53dec6c1a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = compiler.SourceModule(\"\"\"\n",
    "    __global__ void MatrixMultKernel(float *A, float *B, float *C, int Width)\n",
    "    {\n",
    "        float tmp=0;\n",
    "        for(int k=0; k<Width; k++){\n",
    "            tmp += A[threadIdx.y*Width + k] * B[k*Width + threadIdx.x];\n",
    "        }\n",
    "        C[threadIdx.y*Width + threadIdx.x] = tmp;\n",
    "    }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b33232-f95f-4fd2-859f-e6a4b64c581a",
   "metadata": {},
   "source": [
    "If there arenâ€™t any errors, the code is now compiled and loaded onto the device.\n",
    "Now lets define the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f74a40-6e91-4a27-99f5-66cc06acb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumThreads=32\n",
    "NumBlocks = (C.shape[0]+(NumThreads-1))//NumThreads\n",
    "blockdim = (NumThreads,NumThreads)\n",
    "griddim = (NumBlocks,NumBlocks)\n",
    "print(griddim,blockdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71c2e8-cd58-486f-8ba9-4d32f16a5a1f",
   "metadata": {},
   "source": [
    "Now we create matrices A,B,C in CPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba472678-7e43-46b8-a81b-bdc8bafde005",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.rand(512,512).astype(np.float32)\n",
    "B=np.random.rand(512,512).astype(np.float32)\n",
    "C=np.zeros(shape=(512,512)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9950670-72d1-4744-88cc-e5f867a43cd2",
   "metadata": {},
   "source": [
    "Now we allocate GPU memory for the same matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089f4a6-c900-4f20-9a2a-61852104b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_A = cuda.mem_alloc(A.nbytes)\n",
    "d_B = cuda.mem_alloc(B.nbytes)\n",
    "d_C = cuda.mem_alloc(C.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aae04a-4b47-4d29-a6d8-fccde1dc6760",
   "metadata": {},
   "source": [
    "Now we copy data from CPU to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250bea6-f9d7-498a-a8f9-fffabdfe7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.memcpy_htod(d_A, A)\n",
    "cuda.memcpy_htod(d_B, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ddfd4-0089-4782-bc98-131a3b83b80b",
   "metadata": {},
   "source": [
    "We find a reference to our pycuda.driver.Function and call :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cec67-f098-4288-b6df-cf580f3eacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul = mod.get_function(\"MatrixMultKernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31f20b-0e72-480e-bbef-2af01cb5c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit matmul(d_A,d_B,d_C,griddim,blockdim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
