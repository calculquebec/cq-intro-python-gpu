{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sapphire-virgin",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Numba + CUDA: Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1bf0c-ab18-4c6f-85de-09198f33878e",
   "metadata": {},
   "source": [
    "Questions\n",
    "* What is GPU shared memory ?\n",
    "* How to use shared memory ? \n",
    "\n",
    "Objectives\n",
    "* Understand where shared memory is located\n",
    "* Understand how to create arrays of shared memory\n",
    "* Learn to implement shared memory into kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2c420",
   "metadata": {},
   "source": [
    "### What is Shared memory ?\n",
    "It's a memory located on GPU chip, not outside like a global memory. This is why it's extremely fast.\n",
    "\n",
    "* Shared memory latency is 100x lower than that of global memory\n",
    "* Allocated per thread block, so all threads in the block have access to the same shared memory\n",
    "* Threads can access data in shared memory loaded from global memory by other threads within the same thread block\n",
    "\n",
    "\n",
    "Another reason why threads are organized into blocks is to be able to introduce a shared memory concept.\n",
    "\n",
    "A limited amount of shared memory can be allocated on the device to speed up access to data. That memory is shared amongst all threads in a given block. It's so much faster than the regular device memory. It also allows threads to cooperate on a given solution.\n",
    "\n",
    "You can think of it as a manually-managed data cache.\n",
    "\n",
    "Shared memory is also configurable. Shared memory resides in on-chip memory and shares space with L1 cache (registers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc84621-c09b-4c3e-b7dc-cf6a523e748d",
   "metadata": {},
   "source": [
    "### How to allocate shared memory in Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " numba.cuda.shared.array(shape, type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07371739",
   "metadata": {},
   "source": [
    "This function is called on the device, i.e. from the kernel or device function. A common pattern is to have each thread populate one element in the shared array, then wait for all threads to finish using syncthtreads:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab91e0-aaf4-4036-8847-d2c70e53fe00",
   "metadata": {},
   "source": [
    "### Thread synchronization\n",
    "When sharing data between threads, we may need to avoid race condition: e.g. thread A is supposed to read data that is supposed to be written by thread B, but thread B have not finished writing that data and thread A is already trying to access it. \n",
    "\n",
    "To ensure correct results when parallel threads cooperate, we must synchronize the threads. CUDA provides a simple barrier synchronization primitive, __syncthreads(). A threadâ€™s execution can only proceed past a __syncthreads() after all threads in its block have executed the __syncthreads(). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7fa131-ad7f-48a1-909f-cd155222cddf",
   "metadata": {},
   "source": [
    "### How to synchronize threads in Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50deab",
   "metadata": {},
   "outputs": [],
   "source": [
    " numba.cuda.syncthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbff54-ac14-4734-b660-b99809ac8c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main example: Matrix multiplication with shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698337ae-0c7d-44f2-a72d-a1d5cb1befc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](images/05-matmulshared.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb6208-9b50-4509-9e53-371d60a6077d",
   "metadata": {},
   "source": [
    "### What's the Idea ? \n",
    "We use shared memory to re-use global memory data.\n",
    "\n",
    "We decompose our algorithms into 2 phases:\n",
    "\n",
    "    1. Reading data from global memory into tiles\n",
    "    2. Looping over the elements in a tile and performing a dot product \n",
    "    \n",
    "We also need to allocate memory for tiles (TBP,TBP) in shared memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6dbc4b0-4410-4549-a6ea-df99ec94dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f87ede-8d9e-4cfa-894b-1985b78fd81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "def fast_matmul(A, B, C):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    \n",
    "    # Define global and thread indices\n",
    "    \n",
    "    # Define number of blocks per grid\n",
    "    \n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        #####\n",
    "        \n",
    "        # Wait until all threads finish preloading\n",
    "        \n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            #####\n",
    "            \n",
    "        # Wait until all threads finish computing\n",
    "        \n",
    "    # Put tmp into C matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242d1ef-97d2-4261-8333-1711e176a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create matrices A,B,C as numpy arrays (size 128x128). Fill A and B with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979c31a-8ba2-4dd4-a1ca-19209dce299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Calculate number of blocks and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8968f4-c531-4ef9-81ea-87d117cbdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function and time it to get the execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1bd67-8421-4925-bb63-d73f7deb30fa",
   "metadata": {},
   "source": [
    "### Exercise: Array reversal with shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b968561-d26a-4592-81b0-4c7c815aeb61",
   "metadata": {},
   "source": [
    "Here we re-use the code from [previous notebook](04-numba-cuda.ipynb) and add shared memory into play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8ce6bd-75fe-455a-93d6-ec6a6cfaef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91c746-ae84-41ea-83ad-f06dbd6545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take this code and re-write it in the next cell by using a shared memory \n",
    "@cuda.jit\n",
    "def reverseArrayBlock(d_out,d_in):\n",
    "    ind_in = cuda.blockDim.x*cuda.blockIdx.x + cuda.threadIdx.x; ## Index of the current thread\n",
    "    ind_out = cuda.gridsize(1)-ind_in-1 ## Total number of threads - in -1\n",
    "    if ind_in<d_in.size:\n",
    "        d_out[ind_out] = d_in[ind_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84db54f-76c5-482a-8c9d-53fbb07cf2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Here is the code with shared memory\n",
    "@cuda.jit\n",
    "def reverseArrayBlock_shared(d_out,d_in):\n",
    "    # Declare/allocate array s in shared memory\n",
    "    ....\n",
    "    # Create input index\n",
    "    ....\n",
    "    # Populate array s from arrat d_in\n",
    "    ....\n",
    "    # Synchronize threads in each block\n",
    "    ....\n",
    "    # Create output index\n",
    "    ....\n",
    "    if ind_in<d_in.size:\n",
    "        # Populate output array d_out from shared array s\n",
    "        ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afbc0b-2678-430d-a489-f1d259157726",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=256*1000\n",
    "NumThreads=128\n",
    "NumBlocks = (dim + (NumThreads - 1)) // NumThreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba551a3-eb54-48ed-af5c-0e96667f071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create arrays on CPU and GPU (if you want to)\n",
    "a = np.arange(0,dim,dtype=np.int32)\n",
    "b = np.zeros(dim,dtype=np.int32)\n",
    "print(memSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a2fc1-e0b9-4bc1-8d5d-02512be342ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Call the kernel\n",
    "# Static shared memory declaration\n",
    "reverseArrayBlock_shared[NumBlocks,NumThreads](b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dffca0-ec57-4a26-af78-cb356d86fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Modify the kernel as well as the call from the host by changing \n",
    "#        static shared memory declaration to dynamic\n",
    "# Dynamic shared memory declaration\n",
    "reverseArrayBlock_shared[NumBlocks,NumThreads,0,memSize](b,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1315f92-c3c0-413c-9519-27942206a9f9",
   "metadata": {},
   "source": [
    "## Key points\n",
    "* **Numba CUDA Shared memory** \n",
    "    * Device (GPU) won't work without a Host(CPU)\n",
    "    * Both Host and Device have their own memory\n",
    "* **Kernel and Device functions**\n",
    "    * Kernel is declared with @cuda.jit. Kernel is called from  the Host\n",
    "    * Device function is declared with @cuda.jit(device=True) and is called from the Device.\n",
    "* **Explicit data transfers between CPU and GPU**\n",
    "    * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
