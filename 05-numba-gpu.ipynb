{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grateful-sarah",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Numba on GPU\n",
    "Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-worse",
   "metadata": {},
   "source": [
    "### ufuncs\n",
    "A universal function (or ufunc for short) is a function that operates on NumPy arrays (ndarrays) in an element-by-element fashion.\n",
    "A ufunc is a “vectorized” wrapper for a function that takes a fixed number of scalar inputs and produces a fixed number of scalar outputs.\n",
    "\n",
    "Creating a traditional NumPy ufunc is not the most difficult task in the world, but it is also not the most straightforward process and involves writing some C code. Numba makes this easy though. Using the vectorize decorator, Numba can compile a Python function into a ufunc that operates over NumPy arrays as fast as traditional ufuncs written in C.\n",
    "\n",
    "Numba can create compiled ufuncs functions. Just decorate our function with @vectorize.\n",
    "First, let's create an ufunc for the CPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d13a59",
   "metadata": {},
   "source": [
    "### Vectorize decorator and signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "\n",
    "@vectorize\n",
    "def add_n(x, n):\n",
    "    # Done on all elements of ndarray\n",
    "    return x + n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "\n",
    "x = np.arange(n).astype(np.int64)\n",
    "y = np.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-thinking",
   "metadata": {},
   "source": [
    "Here, using @vectorize, you write your function as operating over input scalars, rather than arrays.\n",
    "\n",
    "In order to generate a ufunc for the GPU, you must add an explicit function signature and specify the target. \n",
    "The function signature describes which types are used in input and output in the form of:\n",
    "```python\n",
    "'return_value_type(argument1_value_type, argument2_value_type, ...)'\n",
    "```\n",
    "Below, an addition of two integers that returns an integer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd262687",
   "metadata": {},
   "source": [
    "### CUDA ufuncs\n",
    "With the vectorize decorator you can write a kernel in python, and then have it execute on the GPU.\n",
    "\n",
    "Generating a ufunc that uses CUDA requires giving an explicit type signature and setting the target attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['int64(int64, int64)'], target='cuda')\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run and measure execution time:\n",
    "add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now run the NumPy built-in function and compare:\n",
    "np.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-exhibition",
   "metadata": {},
   "source": [
    "#### Several things happened with this function call:\n",
    "- A CUDA kernel has been created to perform parallel additions on all elements\n",
    "- GPU memory allocation\n",
    "- Moving data to the GPU\n",
    "- Running the kernel\n",
    "- Moving data to the host\n",
    "- Conversion to ndarray\n",
    "- How much faster is our GPU function ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-ratio",
   "metadata": {},
   "source": [
    "### Explicit data management\n",
    "Numba also allows us to manage the movement of our data. Let's take our previous add example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['int32(int32, int32)'], target='cuda')\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, int32\n",
    "d_x=cuda.to_device(x)\n",
    "d_y=cuda.to_device(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and measure execution time:\n",
    "x = np.arange(n).astype(np.int32)\n",
    "y = np.ones_like(x)\n",
    "add(d_x,d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-bunny",
   "metadata": {},
   "source": [
    "Here the result is returned back to CPU. Sometimes you need to leave it on the GPU (e.g. for further computing on GPU). This can be done bty creating an arrar directly on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_res = cuda.device_array(shape=(n,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run again and measure execution time:\n",
    "add(d_x, d_y, out=d_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-chemical",
   "metadata": {},
   "source": [
    "### Multiple signatures\n",
    "It is possible to provide several signatures for the @vectorize :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import vectorize, cuda\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32, float32, float32)',\n",
    "            'float64(float64, float64, float64)'],\n",
    "           target='cuda')\n",
    "def cu_discriminant(a, b, c):\n",
    "    return math.sqrt(b ** 2 - 4 * a * c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
