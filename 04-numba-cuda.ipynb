{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sapphire-virgin",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Numba + CUDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1bf0c-ab18-4c6f-85de-09198f33878e",
   "metadata": {},
   "source": [
    "Questions\n",
    "* How to port CPU code to GPU ?\n",
    "\n",
    "Objectives\n",
    "* Learn how to apply @cuda.jit in Numba CUDA\n",
    "* Learn how to create a CUDA grid in Numba\n",
    "* Understand the GPU memory allocation (implicit or explicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7876d9-6600-4062-9f7b-dbc9033b785f",
   "metadata": {},
   "source": [
    "### Importing Numba CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a725b2-2d3e-4ff1-883c-f326b6cd1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8e63f-1996-4b7f-85be-1e9a43a5962a",
   "metadata": {},
   "source": [
    "### Numba GPU Device Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe0ef5-4d2b-440a-86f3-d5d5830be9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check GPU devices available:\n",
    "cuda.gpus\n",
    "\n",
    "# Select device\n",
    "cuda.select_device(0)\n",
    "\n",
    "#Get some info on the GPU\n",
    "cc_cores_per_SM_dict = {\n",
    "    (2,0) : 32,\n",
    "    (2,1) : 48,\n",
    "    (3,0) : 192,\n",
    "    (3,5) : 192,\n",
    "    (3,7) : 192,\n",
    "    (5,0) : 128,\n",
    "    (5,2) : 128,\n",
    "    (6,0) : 64,\n",
    "    (6,1) : 128,\n",
    "    (7,0) : 64,\n",
    "    (7,5) : 64,\n",
    "    (8,0) : 64,\n",
    "    (8,6) : 128,\n",
    "    (8,9) : 128,\n",
    "    (9,0) : 128\n",
    "    }\n",
    "device = cuda.get_current_device()\n",
    "my_sms = getattr(device, 'MULTIPROCESSOR_COUNT')\n",
    "my_cc = device.compute_capability\n",
    "cores_per_sm = cc_cores_per_SM_dict.get(my_cc)\n",
    "total_cores = cores_per_sm*my_sms\n",
    "print(\"GPU compute capability: \" , my_cc)\n",
    "print(\"GPU total number of SMs: \" , my_sms)\n",
    "print(\"GPU cores per SM: \",cores_per_sm)\n",
    "print(\"GPU total number of cores: \",total_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fa32b-66b6-45d9-b4b6-2cc966d50361",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CUDA kernel declaration in Numba\n",
    "CUDA Kernel is declzred by using @cuda.jit decorator.\n",
    "CUDA Kernel is a function that is called from Host but executed on the Device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \"\"\"\n",
    "    Code for kernel.\n",
    "    \"\"\"\n",
    "    # code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde2148-0644-415d-94c0-18f965b754cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CUDA Device function declaration in Numba\n",
    "CUDA Defice function is a function that is called from Device and executed on the Device\n",
    "Here is how to declare a Device function with the use of @cuda.jit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe9a97-92dd-4597-ae08-693bc0f237cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def my_device_function(io_array):\n",
    "    \"\"\"\n",
    "    Code for Device function.\n",
    "    \"\"\"\n",
    "    # code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512b98c-e774-496e-8d5e-b0f1e9d4a062",
   "metadata": {},
   "source": [
    "### Here is how we create a CUDA grid in Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data array \n",
    "import numpy as np\n",
    "data=np.ones(12800)\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# Now finally start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dedbfe-45b4-44dd-8e3e-1ef017eb0261",
   "metadata": {},
   "source": [
    "What if we need a 2-dimensional blocks, 2-dimensional gird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb89779-b0b6-4c2e-be61-d97a2f7de36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "block = (threadsperblock,threadsperblock)\n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "grid = (blockspergrid,blockspergrid)\n",
    "\n",
    "# Now finally start the kernel\n",
    "my_kernel[grid, block](data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc7c55-8c2e-4123-a2b4-7cfae464d538",
   "metadata": {},
   "source": [
    "### Main example: Matrix multiplication using Numba CUDA (in global memory only)\n",
    "The task is to re-write the function and make it a CUDA kernel with operations in global memory. The idea is to parallelize the problem by distributing the computational load across multiple CUDA threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d04d0-cd87-4c87-bc2f-fa7d19d6774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74110b-a9b1-4ea5-8104-15ce58e07f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(A,B,C):\n",
    "    # iterating by row of A\n",
    "    for i in range(len(A)):\n",
    "  \n",
    "        # iterating by coloum by B \n",
    "        for j in range(len(B[0])):\n",
    "  \n",
    "            # iterating by rows of B\n",
    "            for k in range(len(B)):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346ed42-cbcb-4fb4-b845-309903eb288e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Part 1: Create matrices A,B,C as numpy arrays. Fill A and B with random numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f3cca-223d-445a-ba26-1e0a58e44d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Calculate number of blocks and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcf20f-04ea-414d-9bd5-bbcf53aaa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ea4ef-f234-431b-9f8a-666ad61edca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function and time it to get the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50d177-18e6-45ea-8bbf-501763d6ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Create A,B,C manually on the GPU and copy data to the GPU arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a79488-28be-41e1-80ef-0ee9d759069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Call the kernel function and time it to get the execution time. Compare the execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-quarter",
   "metadata": {},
   "source": [
    "### Exercise: Array elements incrementation\n",
    "In the following exercise each element of an array is incremented : array[i] = array[i] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libs\n",
    "import ...\n",
    "from numba import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a GPU code (Kernel)\n",
    "def kernel1(array):\n",
    "    #define thread index i here ...\n",
    "    if i<array.size:\n",
    "        array[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid: provide with number of blocks and threads per block\n",
    "data=numpy.ones(12800)\n",
    "threads=32\n",
    "blocks ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kernel and measure execution time:\n",
    "kernel1[blocks,threads](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take advatage of excplicit data management and copy an array to GPU before kernel execution. \n",
    "# Then measure the execution time again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7bb8e-7ad0-4871-a7a1-e51ffb2a8d49",
   "metadata": {},
   "source": [
    "### Explicit data management\n",
    "\n",
    "Numba has been automatically transferring the NumPy arrays to the device when you invoke the kernel. However, it can only do so conservatively by always transferring the device memory back to the host when a kernel finishes. To avoid the unnecessary transfer for read-only arrays, it is possible to manually control the transfer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701de911-6b8f-4bb3-9e87-b43b43bc72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.device_array( shape ) #Allocates an empty device ndarray. Similar to numpy.empty()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045d469-fe8b-4118-9023-73139e23c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.to_device( array ) #Copy data from CPU array to GPU array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594a7ea-071e-439c-9199-021ae3711108",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = device_array.copy_to_host() #Copy data back to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175833c-77e6-48fe-bf4f-120674507ee4",
   "metadata": {},
   "source": [
    "Now go back to exercise 1 and modify the code by using the expicit data management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-lighter",
   "metadata": {},
   "source": [
    "### Exercise: Array reversal\n",
    "Here an integer array is sent to GPU where its indices are reversed, i.e. array[0]=array[N-1], array[1]=array[N-2], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator\n",
    "# Kernel: reverse the array content using appropriate indices. \n",
    "# To do so you may need input and output indices. Implement kernel with possibility of multiple thread blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid\n",
    "dim=256*1000\n",
    "NumThreadsPerBlock=\n",
    "NumBlocks = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create arrays on CPU and GPU (if you want to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Initialize host array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Verify the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf82fa-f221-4c2d-9d98-ece1aa77e8ad",
   "metadata": {},
   "source": [
    "## Key points\n",
    "* **Numba @cuda.jit decorator** \n",
    "    * Device (GPU) won't work without a Host(CPU)\n",
    "    * Both Host and Device have their own memory\n",
    "* **Kernel and Device functions**\n",
    "    * Kernel is declared with @cuda.jit. Kernel is called from  the Host\n",
    "    * Device function is declared with @cuda.jit(device=True) and is called from the Device.\n",
    "* **Explicit data transfers between CPU and GPU**\n",
    "    * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
