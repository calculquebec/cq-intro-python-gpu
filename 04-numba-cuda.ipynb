{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sapphire-virgin",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Numba + CUDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1bf0c-ab18-4c6f-85de-09198f33878e",
   "metadata": {},
   "source": [
    "Questions\n",
    "* How to port CPU code to GPU ?\n",
    "\n",
    "Objectives\n",
    "* Learn how to apply @cuda.jit in Numba CUDA\n",
    "* Learn how to create a CUDA grid in Numba\n",
    "* Understand the GPU memory allocation (implicit or explicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7876d9-6600-4062-9f7b-dbc9033b785f",
   "metadata": {},
   "source": [
    "### Importing Numba CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a725b2-2d3e-4ff1-883c-f326b6cd1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8e63f-1996-4b7f-85be-1e9a43a5962a",
   "metadata": {},
   "source": [
    "### Numba GPU Device Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f519f21-ccd3-4d5d-9124-d8544c199231",
   "metadata": {},
   "source": [
    "First check to see whether the GPUs are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9af588-5f74-459e-a7d9-57cb7a1a3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9948f1b-eda5-43bd-bdf9-50c1cee26962",
   "metadata": {},
   "source": [
    "If you have multiple GPUs, then you may need to select one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a6fcf-af77-48e7-9d2d-4202a40d3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe48b060-b168-4fc0-898d-bd851d047a20",
   "metadata": {},
   "source": [
    "You can also get some valuable information about the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe0ef5-4d2b-440a-86f3-d5d5830be9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_cores_per_SM_dict = {\n",
    "    (2,0) : 32,\n",
    "    (2,1) : 48,\n",
    "    (3,0) : 192,\n",
    "    (3,5) : 192,\n",
    "    (3,7) : 192,\n",
    "    (5,0) : 128,\n",
    "    (5,2) : 128,\n",
    "    (6,0) : 64,\n",
    "    (6,1) : 128,\n",
    "    (7,0) : 64,\n",
    "    (7,5) : 64,\n",
    "    (8,0) : 64,\n",
    "    (8,6) : 128,\n",
    "    (8,9) : 128,\n",
    "    (9,0) : 128\n",
    "    }\n",
    "device = cuda.get_current_device()\n",
    "my_sms = getattr(device, 'MULTIPROCESSOR_COUNT')\n",
    "my_cc = device.compute_capability\n",
    "cores_per_sm = cc_cores_per_SM_dict.get(my_cc)\n",
    "total_cores = cores_per_sm*my_sms\n",
    "print(\"GPU compute capability: \" , my_cc)\n",
    "print(\"GPU total number of SMs: \" , my_sms)\n",
    "print(\"GPU cores per SM: \",cores_per_sm)\n",
    "print(\"GPU total number of cores: \",total_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fa32b-66b6-45d9-b4b6-2cc966d50361",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CUDA kernel declaration in Numba\n",
    "CUDA Kernel is declzred by using @cuda.jit decorator.\n",
    "CUDA Kernel is a function that is called from Host but executed on the Device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A,B,C):\n",
    "    \"\"\"\n",
    "    Code for kernel.\n",
    "    \"\"\"\n",
    "    # code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512b98c-e774-496e-8d5e-b0f1e9d4a062",
   "metadata": {},
   "source": [
    "### How to create a CUDA grid for matrix multiplication\n",
    "Similarly to a parallelization we did in a previous chapter, here we need to distribute a computational load among available CUDA threads. It's just this time we don't loop but rather create enough threads so that each thread does exactly one matrix element. There are several ways of dsoing so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178e792-2602-44da-8693-63ea389a20f1",
   "metadata": {},
   "source": [
    "#### 1. Using a single thread block\n",
    "Given that matrtix is a 2-dimensional object, it makes sense to create a 2-dimensional CUDA block.\n",
    "\n",
    "You can think of CUDA threads as workers.\n",
    "Here we request one single CUDA block of (NumThreads x NumThreads) workers. \n",
    "The matrix elements are distributed among (or assigned to) those workers. Each matrix element is computed independently by a single thread-worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of threads per block (Rule of thumb: 32-512)\n",
    "NumThreads = 32\n",
    "NumBlocks = 1\n",
    "griddim = (NumBlocks,NumBlocks)\n",
    "blockdim = (NumThreads,NumThreads)\n",
    "print(griddim)\n",
    "print(blockdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa630d6-2401-4226-b60b-b2092dafd9ea",
   "metadata": {},
   "source": [
    "Here our CUDA grid consist of one block. Inside that block there are threads x threads.\n",
    "\n",
    "Limitation: Each thread block can fit only 1024 threads - thus we can only do a matrix of size (32x32)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db5983-d4c1-4fea-91e5-00ca8932e6fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Using multiple thread blocks\n",
    "If the size of the matrix is > 32, we need multiple CUDA blocks in both rows and column axes. \n",
    "In other words, we request a 2-dimensional CUDA grid of blocks of (NumThreads x NumThreads) workers enough to cover the whole matrix. Number of blocks depends on the size of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb464d50-bf1b-4e2e-94b5-908405ea8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.rand(128,128).astype(np.float32)\n",
    "B=np.random.rand(128,128).astype(np.float32)\n",
    "C=np.zeros(shape=(128,128)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d4fe9-e46c-43db-851d-a890d0af836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption of a square matrix\n",
    "\n",
    "threads = 32\n",
    "blocks = (int)(C.shape[0]//threads)\n",
    "griddim = (blocks,blocks)\n",
    "blockdim = (threads,threads)\n",
    "print(griddim)\n",
    "print(blockdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9045bc8-b1fe-4a92-a441-f991cf8b3301",
   "metadata": {},
   "source": [
    "![](images/multiplication_multiple_blocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7692b55-93ad-424c-bff9-44467c07780d",
   "metadata": {},
   "source": [
    "Each thread computes one element of C:\n",
    "* Loads a row of matrix A\n",
    "* Load a column of matrix B\n",
    "* Computes a dot product\n",
    "\n",
    "Every value of A and B is loaded N times from global memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98c0e7-77c8-4afe-85fc-6baa961c34d2",
   "metadata": {},
   "source": [
    "### How to call a kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dd855-a529-4e52-9ad3-91bbae2e55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul[griddim,blockdim](A,B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc7c55-8c2e-4123-a2b4-7cfae464d538",
   "metadata": {},
   "source": [
    "### Main example: Matrix multiplication using Numba CUDA (in global memory only)\n",
    "The task is to re-write the function and make it a CUDA kernel with operations in global memory. The idea is to parallelize the problem by distributing the computational load across multiple CUDA threads. Here we can try a single and multiple CUDA blocks approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d04d0-cd87-4c87-bc2f-fa7d19d6774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74110b-a9b1-4ea5-8104-15ce58e07f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(A,B,C):\n",
    "    # iterating by row of A\n",
    "    for i in range(len(A)):\n",
    "  \n",
    "        # iterating by coloum by B \n",
    "        for j in range(len(B[0])):\n",
    "  \n",
    "            # iterating by rows of B\n",
    "            for k in range(len(B)):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346ed42-cbcb-4fb4-b845-309903eb288e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Part 1: Create matrices A,B,C as numpy arrays. Fill A and B with random numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f3cca-223d-445a-ba26-1e0a58e44d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Calculate number of blocks and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcf20f-04ea-414d-9bd5-bbcf53aaa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ea4ef-f234-431b-9f8a-666ad61edca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function and time it to get the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50d177-18e6-45ea-8bbf-501763d6ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Create A,B,C manually on the GPU and copy data to the GPU arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a79488-28be-41e1-80ef-0ee9d759069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Call the kernel function and time it to get the execution time. Compare the execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155216ca-3aa7-4ed5-b323-3ae4746c3d8d",
   "metadata": {},
   "source": [
    "### Explicit data management\n",
    "\n",
    "Numba has been automatically transferring the NumPy arrays to the device when you invoke the kernel. However, it can only do so conservatively by always transferring the device memory back to the host when a kernel finishes. To avoid the unnecessary transfer for read-only arrays, it is possible to manually control the transfer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cfba38-dbe0-453c-9abe-f5ec477e3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.device_array( shape ) #Allocates an empty device ndarray. Similar to numpy.empty()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c8b78-92f1-4c43-a94b-f8eac364b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.to_device( array ) #Copy data from CPU array to GPU array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55672740-be6f-44cd-ab1c-a9da295e246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = device_array.copy_to_host() #Copy data back to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddfb6d-333f-4d49-b458-fab14da6801a",
   "metadata": {},
   "source": [
    "Now go back to Matrix multiplication exercise and modify the code by using the expicit data management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-quarter",
   "metadata": {},
   "source": [
    "### Exercise: Incrementation of array elements\n",
    "In the following exercise each element of an array is incremented : array[i] = array[i] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libs\n",
    "import ...\n",
    "from numba import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a GPU code (Kernel)\n",
    "def increment(array):\n",
    "    #define thread index i here ...\n",
    "    if i<array.size:\n",
    "        array[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid: provide with number of blocks and threads per block\n",
    "data=numpy.ones(12800)\n",
    "NumThreads=32\n",
    "NumBlocks ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kernel and measure execution time:\n",
    "increment[NumBlocks,NumThreads](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take advatage of excplicit data management and copy an array to GPU before kernel execution. \n",
    "# Then measure the execution time again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-lighter",
   "metadata": {},
   "source": [
    "### Exercise: Reversal of array elements\n",
    "Here an integer array is sent to GPU where its indices are reversed, i.e. array[0]=array[N-1], array[1]=array[N-2], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator\n",
    "# Kernel: reverse the array content using appropriate indices. \n",
    "# To do so you may need input and output indices. Implement kernel with possibility of multiple thread blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid\n",
    "dim=256*1000\n",
    "NumThreads=\n",
    "NumBlocks = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create arrays on CPU and GPU (if you want to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Initialize host array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Verify the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf82fa-f221-4c2d-9d98-ece1aa77e8ad",
   "metadata": {},
   "source": [
    "## Key points\n",
    "* **Numba @cuda.jit decorator** \n",
    "    * Device (GPU) won't work without a Host(CPU)\n",
    "    * Both Host and Device have their own memory\n",
    "* **Kernel and Device functions**\n",
    "    * Kernel is declared with @cuda.jit. Kernel is called from  the Host\n",
    "    * Device function is declared with @cuda.jit(device=True) and is called from the Device.\n",
    "* **Explicit data transfers between CPU and GPU**\n",
    "    * Data arrays can be allocated on GPU\n",
    "    * Data can be copied manually to GPU/CPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
