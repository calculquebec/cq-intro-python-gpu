{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sapphire-virgin",
   "metadata": {},
   "source": [
    "# Introduction to GPU Programming with Python\n",
    "## Numba + CUDA\n",
    "\n",
    "Universal functions are great for element wise operations. However, not all operations are element wise. To compile a function on the GPU that is not element wise, we must use `numba.cuda.jit`.\n",
    "\n",
    "\n",
    "Several important terms in the topic of CUDA programming are listed here:\n",
    "\n",
    "- host: the CPU\n",
    "\n",
    "- device: the GPU\n",
    "\n",
    "- host memory: the system main memory\n",
    "\n",
    "- device memory: onboard memory on a GPU card\n",
    "\n",
    "- kernels: a GPU function launched by the host and executed on the device\n",
    "\n",
    "- device function: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7917cb1",
   "metadata": {},
   "source": [
    "### CPU vs. GPU\n",
    "\n",
    "CPUs are optimized for latency.\n",
    "\n",
    "A CPU tries to execute a given instruction as quickly as possible, i.e., it tries to keep the latency (the time between issuing and executing an instruction) as short as possible. CPUs use caches and a lot of control logic to achieve this goal.\n",
    "\n",
    "GPUs are optimized for throughput.\n",
    "\n",
    "GPUs were (and are) made to display graphics on your screen. It doesn't matter how quickly a GPU can update a single pixel. It's important how quickly it can update all of the pixels on the screen (more than 2 million on an HD display). In addition it often must perform the same operation on a lot of vertices or pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-prompt",
   "metadata": {},
   "source": [
    "### Short intro to CUDA\n",
    "\n",
    "CUDA - Compute Unified Device Architecture\n",
    "\n",
    "Provides access to instructions and memory of massively parallel elements in CUDA-enabled GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-knitting",
   "metadata": {},
   "source": [
    "### GPU Architecture\n",
    "![](images/GPU-architecture.png)\n",
    "\n",
    "#### <font color='blue'>CUDA sees GPU as: </font>\n",
    "- lots of streaming microprocessors(SMs)\n",
    "- separate GPU memory\n",
    "- scheduler\n",
    "- thousands of compute threads (similar to CPU compute threads)\n",
    "\n",
    "#### <font color='blue'>Thousands of threads attack the same code </font>\n",
    "![](images/threads_attack.png)\n",
    "\n",
    "#### <font color='blue'>To manage this many threads we need: </font>\n",
    "- to ID each thread\n",
    "- organize these threads\n",
    "- schedule them for execution\n",
    "\n",
    "#### <font color='blue'>How threads are organized: </font>\n",
    "- threads within a bock cooperate (exchange data via shared memory)\n",
    "- threads in different blocks can not cooperate\n",
    "\n",
    "#### <font color='blue'>Threads layout: </font>\n",
    "- threads are organized into blocks\n",
    "- blocks are organized into a grid\n",
    "- SM executes one block at a time\n",
    "\n",
    "#### <font color='blue'>Simple CUDA program run on GPU threads is called </font><font color='red'>KERNEL </font>\n",
    "![](images/block_grid.png)\n",
    "\n",
    "### CUDA Programming Recepie\n",
    "- copy input data from CPU memory to GPU memory\n",
    "- load GPU program (KERNEL) and execute\n",
    "- copy results from the GPU memory back to the CPU memory\n",
    "### GPU execution model\n",
    "\n",
    "GPUs use many lightweight threads.\n",
    "GPUs hide latency instead of avoiding it\n",
    "GPUs work best if the problem can me mapped on a grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-pension",
   "metadata": {},
   "source": [
    "### CUDA Block-Threading model\n",
    "#### Thread positioning:\n",
    "To help deal with multi-dimensional arrays, CUDA allows you to specify multi-dimensional blocks and grids. In the example above, you could make blockspergrid and threadsperblock tuples of one, two or three integers. Compared to 1-dimensional declarations of equivalent sizes, this doesn’t change anything to the efficiency or behaviour of generated code, but can help you write your algorithms in a more natural way.\n",
    "\n",
    "In CUDA the following objects are defined: threadIdx.x, blockIdx.x, blockDim.x, gridDim.x . They help tp know the thread’s hierarchy.\n",
    "\n",
    "\n",
    "#### Absolute positions:\n",
    "- cuda.grid (ndim) : returns absolute position of the current thread in the entire grid\n",
    "- cuda.gridsize (ndim) : returns absolute size in threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-programmer",
   "metadata": {},
   "source": [
    "In the previous examples the operations on arrays were uploaded to GPU which parallelized that operations automatically because we used @vectorize decorator which distributed the calculation among many GPU threads. However, it's not always the case and we need to have an explicit control over the threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e3399",
   "metadata": {},
   "source": [
    "### Device Management\n",
    "Before doing any computing we need to find available GPUs and choose one.\n",
    "It is possible to obtain a list of all the GPUs in the system using the following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad7fe9",
   "metadata": {},
   "source": [
    "If your machine has multiple GPUs, you might want to select which one to use. By default the CUDA driver selects the fastest GPU as the device 0, which is the default device used by Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id=0\n",
    "cuda.select_device( device_id )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986fc18",
   "metadata": {},
   "source": [
    "This creates a new CUDA context for the selected device_id. device_id should be the number of the device (starting from 0; the device order is determined by the CUDA libraries). The context is associated with the current thread. Numba currently allows only one context per thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d9ee4",
   "metadata": {},
   "source": [
    "### Writing CUDA kernels\n",
    "In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). Your solution will be modeled by defining a thread hierarchy of grid, blocks, and threads.\n",
    "Numba also exposes three kinds of GPU memory:\n",
    "\n",
    "    global device memory\n",
    "    shared memory\n",
    "    local memory\n",
    "NVIDIA recommends :\n",
    "\n",
    "    Find ways to parallelize sequential code\n",
    "    Minimize data transfers between the host and the device\n",
    "    Adjust kernel launch configuration to maximize device utilization\n",
    "    Ensure global memory accesses are coalesced\n",
    "    Minimize redundant accesses to global memory whenever possible\n",
    "    Avoid different execution paths within the same warp\n",
    "#### Kernel declaration\n",
    "A kernel function is a GPU function that is meant to be called from CPU code. It has two fundamental characteristics:\n",
    "\n",
    "    kernels cannot explicitly return a value; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array);\n",
    "    kernels explicitly declare their thread hierarchy when called: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \"\"\"\n",
    "    Code for kernel.\n",
    "    \"\"\"\n",
    "    # code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data array \n",
    "data=numpy.ones(12800)\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# Now finally start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aeeb05",
   "metadata": {},
   "source": [
    "### Choosing the block size\n",
    "\n",
    "* On the software side, the block size determines how many threads share a given area of shared memory.\n",
    "* On the hardware side, the block size must be large enough for full occupation of execution units; \n",
    "The block size you choose depends on:\n",
    "* The size of the data array\n",
    "* The size of the shared mempory per block (e.g. 64KB)\n",
    "* The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)\n",
    "* The maximum number of threads per multiprocessor (MP) (e.g. 2048)\n",
    "* The maximum number of blocks per MP (e.g. 32)\n",
    "* The number of threads that can be executed concurrently (a “warp” i.e. 32)\n",
    "\n",
    "Rules of thumb for threads per block:\n",
    "\n",
    "    Should be a round multiple of the warp size (32)\n",
    "    A good place to start is 128-512 but benchmarking is required to determine the optimal value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136156c4",
   "metadata": {},
   "source": [
    "### Thread positioning in Numba\n",
    "Numba uses similar to CUDA syntax to address the thread positioning\n",
    "- cuda.threadIdx.x, cuda.blockIdx.x,   cuda.blockDim.x, cuda.gridDim.x are special objects provided by CUDA to know\n",
    "These objects can be 1-, 2- or 3-dimensional, depending on how the kernel was invoked. To access the value at each dimension, use the x, y and z attributes of these objects, respectively.\n",
    "\n",
    "### Absolute positions\n",
    "\n",
    "Simple algorithms will tend to always use thread indices in the same way as shown in the example above. Numba provides additional facilities to automate such calculations:\n",
    "\n",
    "* numba.cuda.grid(ndim) - Return the absolute position of the current thread in the entire grid of blocks. ndim should correspond to the number of dimensions declared when instantiating the kernel. If ndim is 1, a single integer is returned. If ndim is 2 or 3, a tuple of the given number of integers is returned.\n",
    "* numba.cuda.gridsize(ndim) - Return the absolute size (or shape) in threads of the entire grid of blocks. ndim has the same meaning as in grid() above.\n",
    "order to know which array element(s) it is responsible for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-quarter",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "Lets do the following exercise where each element of an array is incremented : array[i] = array[i] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libs\n",
    "import ...\n",
    "from numba import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a GPU code (Kernel)\n",
    "def kernel1(array):\n",
    "    #define thread index i here ...\n",
    "    if i<array.size:\n",
    "        array[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid: provide with number of blocks and threads per block\n",
    "data=numpy.ones(12800)\n",
    "threads=32\n",
    "blocks ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kernel and measure execution time:\n",
    "kernel1[blocks,threads](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take advatage of excplicit data management and copy an array to GPU before kernel execution. \n",
    "# Then measure the execution time again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-lighter",
   "metadata": {},
   "source": [
    "#### Exercice 2\n",
    "Integer array, sent to GPU where its indices are reversed, i.e. array[0]=array[N-1], array[1]=array[N-2], etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import numpy as np\n",
    "from numba import cuda, float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator\n",
    "# Kernel: reverse the array content using appropriate indices. \n",
    "# To do so you may need input and output indices. Implement kernel with possibility of multiple thread blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA grid\n",
    "dim=256\n",
    "NumBlocks=1\n",
    "NumThreadsPerBlock=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Create arrays on CPU and GPU (if you want to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Initialize host array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Verify the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-drunk",
   "metadata": {},
   "source": [
    "#### Exercice 3\n",
    "Repeat Excercise 2 with multiple blocks per CUDA grid (NumBlocks > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-florence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-isaac",
   "metadata": {},
   "source": [
    "### Explicit data management\n",
    "\n",
    "Numba has been automatically transferring the NumPy arrays to the device when you invoke the kernel. However, it can only do so conservatively by always transferring the device memory back to the host when a kernel finishes. To avoid the unnecessary transfer for read-only arrays, it is possible to manually control the transfer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.device_array( shape ) #Allocates an empty device ndarray. Similar to numpy.empty()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = cuda.to_device( array ) #Copy data from CPU array to GPU array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = device_array.copy_to_host() #Copy data back to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfc421",
   "metadata": {},
   "source": [
    "In the below example we try to avoid automatic data transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b7408-ebf0-49c4-ade2-962016dab078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['int32(int32, int32)'], target='cuda')\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "n = 10\n",
    "x=np.arange(n).astype(np.int32)\n",
    "y=np.ones_like(x)\n",
    "\n",
    "d_x=cuda.to_device(x)\n",
    "d_y=cuda.to_device(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and measure execution time:\n",
    "add(d_x,d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-proposal",
   "metadata": {},
   "source": [
    "Here the result is returned back to CPU. Sometimes you need to leave it on the GPU (e.g. for further computing on GPU). This can be done bty creating an arrar directly on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_res = cuda.device_array(shape=(n,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run again and measure execution time:\n",
    "add(d_x, d_y, out=d_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-penny",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "legislative-light",
   "metadata": {},
   "source": [
    "### Calling device funcs\n",
    "All the functions we created so far are run on GPU but called from the CPU. Sometimes it's needed to have a function callable from the GPU only. It can be done by adding an extra recipe \"device=True\" to @jit decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize, cuda\n",
    "\n",
    "@cuda.jit('float32(float32, float32, float32)', device=True, inline=True)\n",
    "def cu_device_fn(x, y, z):\n",
    "    return x ** y / z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-advisory",
   "metadata": {},
   "source": [
    "Then we create a function ( callable from the CPU ) which calls the above GPU function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
    "def cu_ufunc(x, y, z):\n",
    "    return cu_device_fn(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def device_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def do_sum_pow(v1, v2):\n",
    "    s1 = device_add(v1, v2)\n",
    "    return s1 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "v1 = np.random.uniform(0.5, 1.5, size=n).astype(np.float32)\n",
    "v2 = np.random.uniform(2.5, 5.5, size=n).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_sum_pow(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-request",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "Polynomial evaluation on both GPU and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Modify polynomial function to make it work with numba.cuda\n",
    "def host_polyval(result, array, coeffs):\n",
    "    for i in range(len(array)):\n",
    "        val = coeffs[0]\n",
    "        for coeff in coeffs[1:]:\n",
    "            val = val * array[i] + coeff\n",
    "        result[i] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44600dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Allocate integer array (int32), size of 2048 * 1024. Also make an empty array for result, same size\n",
    "array = \n",
    "coeffs = np.float32(range(1, 10))\n",
    "result = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Prepare grid\n",
    "blocks=\n",
    "threads="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel and measure execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c305196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Call the built-in NumPy polynomial function  np.polyval(coeffs, array) and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Go back to the kernel (Part 3) and modify it to make it work on CPU with @jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06a577",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "Matrix multiplication WITH GLOBAL MEMORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b242200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(A,B,C):\n",
    "    # iterating by row of A\n",
    "    for i in range(len(A)):\n",
    "  \n",
    "        # iterating by coloum by B \n",
    "        for j in range(len(B[0])):\n",
    "  \n",
    "            # iterating by rows of B\n",
    "            for k in range(len(B)):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f7372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Part 1: Create matrices A,B,C as numpy arrays. Fill A and B with random numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da28fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Calculate number of blocks and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Create a CUDA kernel with @cuda.jit decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d64235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Call the kernel function and time it to get the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Create A,B,C manually on the GPU and copy data to the GPU arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Call the kernel function and time it to get the execution time. Compare the execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2c420",
   "metadata": {},
   "source": [
    "### Shared memory\n",
    "A limited amount of shared memory can be allocated on the device to speed up access to data. That memory is shared amongst all threads in a given block. It's so much faster than the regular device memory. It also allows threads to cooperate on a given solution.\n",
    "The memory is allocated once for the duration of the kernel, unlike traditional dynamic memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " numba.cuda.shared.array(shape, type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07371739",
   "metadata": {},
   "source": [
    "This function is called on the device, i.e. from the kernel or device function. A common pattern is to have each thread populate one element in the shared array, then wait for all threads to finish using syncthtreads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50deab",
   "metadata": {},
   "outputs": [],
   "source": [
    " numba.cuda.syncthreads()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
